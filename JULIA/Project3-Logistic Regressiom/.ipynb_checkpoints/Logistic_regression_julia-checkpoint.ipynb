{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3  - Logistic Regression - using Julia\n",
    "## Nhi Le\n",
    "## DATA 4319 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression uses logistic sigmoid activation, in contrast to linear regression, which\n",
    "uses the identity function. As we've seen before, the output of the logistic sigmoid is in the\n",
    "(0,1) range and can be interpreted as a probability function. We can use logistic regression\n",
    "for a 2-class (binary) classification problem, where our target, t, can have two values,\n",
    "usually 0 and 1 for the two corresponding classes. These discrete values shouldn't be\n",
    "confused with the values of the logistic sigmoid function, which is a continuous real-valued\n",
    "function between 0 and 1. The value of the sigmoid function represents the probability that\n",
    "the output is in class 0 or class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic function or logistic curve is a common S-shaped curve (sigmoid curve) with equation\n",
    "\n",
    "$${\\displaystyle f(x)={\\frac {L}{1+e^{-k(x-x_{0})}}},}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, logistic regression is a predictive analysis that used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Here is the basic formula of logistic regression:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/c5e464fcd1955db626a19adf846bfb57ab5007e607b040e8f07ac9f579c8a5a1/687474703a2f2f666163756c74792e6361732e7573662e6564752f6d6272616e6e69636b2f72656772657373696f6e2f676966732f6c6f382e676966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Standard logistic sigmoid function i.e. L = 1, k = 0, x0 = 0_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Vs. Logistic Regression\n",
    "Linear regression gives you a continuous output, but logistic regression provides a constant output. An example of the continuous output is house price and stock price. Example's of the discrete output is predicting whether a patient has cancer or not, predicting whether the customer will churn. Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Logistic Regression:\n",
    "\n",
    "**Binary Logistic Regression:** The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n",
    "\n",
    "**Multinomial Logistic Regression:** The target variable has three or more nominal categories such as predicting the type of Wine.\n",
    "\n",
    "**Ordinal Logistic Regression:** The target variable has three or more ordinal categories such as restaurant or product rating from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>40 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>690</td><td>1.7</td><td>1</td><td>0</td></tr><tr><th>11</th><td>610</td><td>2.7</td><td>3</td><td>0</td></tr><tr><th>12</th><td>690</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>13</th><td>710</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>14</th><td>680</td><td>3.3</td><td>4</td><td>0</td></tr><tr><th>15</th><td>770</td><td>3.3</td><td>3</td><td>1</td></tr><tr><th>16</th><td>610</td><td>3.0</td><td>1</td><td>0</td></tr><tr><th>17</th><td>580</td><td>2.7</td><td>4</td><td>0</td></tr><tr><th>18</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>19</th><td>540</td><td>2.7</td><td>2</td><td>0</td></tr><tr><th>20</th><td>590</td><td>2.3</td><td>3</td><td>0</td></tr><tr><th>21</th><td>620</td><td>3.3</td><td>2</td><td>1</td></tr><tr><th>22</th><td>600</td><td>2.0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>550</td><td>2.3</td><td>4</td><td>0</td></tr><tr><th>24</th><td>550</td><td>2.7</td><td>1</td><td>0</td></tr><tr><th>25</th><td>570</td><td>3.0</td><td>2</td><td>0</td></tr><tr><th>26</th><td>670</td><td>3.3</td><td>6</td><td>1</td></tr><tr><th>27</th><td>660</td><td>3.7</td><td>4</td><td>1</td></tr><tr><th>28</th><td>580</td><td>2.3</td><td>2</td><td>0</td></tr><tr><th>29</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>30</th><td>660</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\t11 & 610 & 2.7 & 3 & 0 \\\\\n",
       "\t12 & 690 & 3.7 & 5 & 1 \\\\\n",
       "\t13 & 710 & 3.7 & 6 & 1 \\\\\n",
       "\t14 & 680 & 3.3 & 4 & 0 \\\\\n",
       "\t15 & 770 & 3.3 & 3 & 1 \\\\\n",
       "\t16 & 610 & 3.0 & 1 & 0 \\\\\n",
       "\t17 & 580 & 2.7 & 4 & 0 \\\\\n",
       "\t18 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t19 & 540 & 2.7 & 2 & 0 \\\\\n",
       "\t20 & 590 & 2.3 & 3 & 0 \\\\\n",
       "\t21 & 620 & 3.3 & 2 & 1 \\\\\n",
       "\t22 & 600 & 2.0 & 1 & 0 \\\\\n",
       "\t23 & 550 & 2.3 & 4 & 0 \\\\\n",
       "\t24 & 550 & 2.7 & 1 & 0 \\\\\n",
       "\t25 & 570 & 3.0 & 2 & 0 \\\\\n",
       "\t26 & 670 & 3.3 & 6 & 1 \\\\\n",
       "\t27 & 660 & 3.7 & 4 & 1 \\\\\n",
       "\t28 & 580 & 2.3 & 2 & 0 \\\\\n",
       "\t29 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t30 & 660 & 3.3 & 5 & 1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "40×4 DataFrame\n",
       "│ Row │ gmat  │ gpa     │ work_experience │ admitted │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m           │ \u001b[90mInt64\u001b[39m    │\n",
       "├─────┼───────┼─────────┼─────────────────┼──────────┤\n",
       "│ 1   │ 780   │ 4.0     │ 3               │ 1        │\n",
       "│ 2   │ 750   │ 3.9     │ 4               │ 1        │\n",
       "│ 3   │ 690   │ 3.3     │ 3               │ 0        │\n",
       "│ 4   │ 710   │ 3.7     │ 5               │ 1        │\n",
       "│ 5   │ 680   │ 3.9     │ 4               │ 0        │\n",
       "│ 6   │ 730   │ 3.7     │ 6               │ 1        │\n",
       "│ 7   │ 690   │ 2.3     │ 1               │ 0        │\n",
       "│ 8   │ 720   │ 3.3     │ 4               │ 1        │\n",
       "│ 9   │ 740   │ 3.3     │ 5               │ 1        │\n",
       "│ 10  │ 690   │ 1.7     │ 1               │ 0        │\n",
       "⋮\n",
       "│ 30  │ 660   │ 3.3     │ 5               │ 1        │\n",
       "│ 31  │ 640   │ 3.0     │ 1               │ 0        │\n",
       "│ 32  │ 620   │ 2.7     │ 2               │ 0        │\n",
       "│ 33  │ 660   │ 4.0     │ 4               │ 1        │\n",
       "│ 34  │ 660   │ 3.3     │ 6               │ 1        │\n",
       "│ 35  │ 680   │ 3.3     │ 5               │ 1        │\n",
       "│ 36  │ 650   │ 2.3     │ 1               │ 0        │\n",
       "│ 37  │ 670   │ 2.7     │ 2               │ 0        │\n",
       "│ 38  │ 580   │ 3.3     │ 1               │ 0        │\n",
       "│ 39  │ 590   │ 1.7     │ 4               │ 0        │\n",
       "│ 40  │ 690   │ 3.7     │ 5               │ 1        │"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "using CSV\n",
    "data= CSV.read(\"candidates_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Feature\n",
    "Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=[[x[1],x[2]] for x in zip(data.gmat,data.gpa)]\n",
    "y_data=[x for x in data.admitted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing logistic regression**\n",
    "\n",
    "$$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1/(1+exp(-x))\n",
    "\n",
    "function cross_entropy_loss(x,y,w,b)\n",
    "    return -y*log(σ(w'x + b)) - (1-y)*log(1-σ(w'x+b))\n",
    "end \n",
    "\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i],w,b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features,labels,w,b,α)\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    N = length(features)\n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i] + b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i] + b) - labels[i])\n",
    "    end\n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.012, 0.0006000000000000002], -0.0001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_batch_gradient_descent(features,labels, w,b,α, epochs)\n",
    "    for i = 1:epochs \n",
    "        \n",
    "        w, b = batch_gradient_descent(features, labels, w,b,α)\n",
    "        if i == 1\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/10\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/8\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/4\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/2\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        end \n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 with loss 0.6931188566349795\n",
      "Epochs 100000 with loss 0.6855799117618873\n",
      "Epochs 125000 with loss 0.6837589079497152\n",
      "Epochs 250000 with loss 0.6749998518952888\n",
      "Epochs 500000 with loss 0.6590868605720882\n",
      "Epochs 1000000 with loss 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.0,0.0]\n",
    "b = 0.0\n",
    "\n",
    "w,b = train_batch_gradient_descent(x_data,y_data, w,b,0.0000001,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        println(\"predict accepted\")\n",
    "        y==1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "    else\n",
    "        println(\"predict not accepted\")\n",
    "        y==1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was not accepted\n",
      "\n",
      "predict not accepted\n",
      "was not accepted\n",
      "\n",
      "predict accepted\n",
      "was accepted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i =1:length(x_data)\n",
    "    predict(x_data[i],y_data[i],w,b)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "print(mean_error/length(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Conclusion:\n",
    "The Logistic Regression model can accurately predict if someone will be admitted based on the data and result using the combination of their GPA, GMAT and amount of work experience\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
